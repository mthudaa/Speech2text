{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mthudaa/Speech2text/blob/main/_notebooks/2020-08-20-mfcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLpv8ZuKnqXA"
      },
      "source": [
        "# What, how, and why of MFCCs\n",
        "> In the quest for compact representation of speech\n",
        "\n",
        "- toc: False\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [coswara, tutorial]\n",
        "- image: images/chart-preview.png\n",
        "- author: Neeraj Sharma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVyNDmjYnqXD"
      },
      "source": [
        "MFCC stands for mel-frequency cepstral coefficient. In this tutorial we will understand the significance of each word in the acronym, and *how* these terms are put together to create a signal processing pipeline for acoustic feature extraction. The resulting features, MFCCs, are quite popular for speech and audio R&D. Why so? We will have an answer for this by the end of this notebook.\n",
        "\n",
        "## Say hi to our signal\n",
        "\n",
        "It is good to understand by doing and hence, we will carry the below speech signal alongside us. Do listen to eat before we proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cIH1-KoAnqXE",
        "outputId": "59a6a548-fe07-4137-ee4c-d350e9e13c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-62e0c36e0f69>:17: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  x, sr = librosa.load(dname+fname,sr=fs)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_normal_count.wav'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Otherwise, create the soundfile object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    657\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_normal_count.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-62e0c36e0f69>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./my_data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 )\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-157>\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36m__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Would be 2, but the decorator adds a level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# If the input was not an audioread object, try to open it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './my_data/nMIOAh7qRFf3pqbchclOLKbPDOm1_normal_count.wav'"
          ]
        }
      ],
      "source": [
        "#collapse\n",
        "import numpy as np\n",
        "import librosa\n",
        "from IPython.lib.display import Audio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
        "                               AutoMinorLocator)\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "fs = 16000\n",
        "fname = 'nMIOAh7qRFf3pqbchclOLKbPDOm1_normal_count.wav'\n",
        "dname = './my_data/'\n",
        "# load\n",
        "x, sr = librosa.load(dname+fname,sr=fs)\n",
        "x = x/max(np.abs(x))\n",
        "times = np.arange(0,len(x))/sr\n",
        "# listen\n",
        "Audio(x, rate=sr, autoplay=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R1HUgSznqXG"
      },
      "source": [
        "Below is the time-domain representation. The duration of this signal is 3.64 seconds, and it has 58378 samples (or data points). Our goal is to represent these 58378 data points with fewer numbers (or a compact representation) and still preserve the essential features of the signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKQPMdjxnqXG"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "# plot\n",
        "print('Nos. samples: '+str(len(x)))\n",
        "print('Duration: '+str(times[-1])+ 'seconds')\n",
        "\n",
        "fig = plt.subplots(figsize=(12,2))\n",
        "ax = plt.subplot(1,1,1)\n",
        "ax.plot(times, x)\n",
        "ax.grid(True)\n",
        "plt.ylabel('amplitude [in A.U.]', fontsize=14)\n",
        "plt.xlabel('time [in sec]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCqcnwl-nqXG"
      },
      "source": [
        "## Signal non-stationarity\n",
        "\n",
        "> If you plot a speech signal waveform you will note that the waveform shape changes significantly over time. This time is not in months or years but in tens of milliseconds. You don't believe, right? See below image.\n",
        "It depicts a speech signal. From the signal we take a 25 msec segment, compute its magnitude Fourier transform, and push the output into a column of an empty matrix. Then we hop by 10 msec along the speech signal (from left to right), and again take a 25 msec segment, and repeat the procedure - magnitude fourier transform, push into the column of the matrix, and hop, till we reach the end of the speech signal. The resulting matrix is referred to as the spectrogram. We have plotted this matrix as an image (color bar: color gradient from dark blue to white indicates high to low amplitude in the spectral content). Spend some time staring at the spectrogram and you would notice the changing spectrum across time! In a more technical sense, this changing spectral content over time behavior of sound signals is referred to as signal non-stationarity. If you think philosophically, a reason we enjoy speech and music signals is due the non-stationarity in these signals. A sinusoid (like, a tone signal) will bore us very quickly.\n",
        "\n",
        "![](./my_images/spectrogramIllus.png \"Credit: http://coswara.iisc.ac.in/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBNU-VLYnqXH"
      },
      "source": [
        "The worrying thing with signal non-stationarity is that the majority of statistics and signal processing methods are applicable to stationary signals. For instance, the methods assume that the underlying data distribution does not change over time (Ergodic process) or the Fourier spectrum stays the same. To apply these methods to speech and audio signals we will make use of short-time analysis. We will segment the audio signal into short-time segments (of duration ``windDur``), and from each segment we will extract some compact representation. An illustration is shown in the below figure. Here, a 3-D representation is extracted from every short-time segment.\n",
        "![](./my_images/short_time_features.png \" An illustration on short-time features. Here each frame is represented with a 3-D feature vector. Credit:  http://coswara.iisc.ac.in/\")\n",
        "\n",
        "### Windowing\n",
        "When we take out a short-time segment from the full signal, the start and end of the short-time segment signal may have abrupt discontinuity. This will introduce spurious frequencies in the Fourier transform of the  signal (more [here](https://ccrma.stanford.edu/~jos/sasp/Effect_Windowing.html)). Hence, it is a usual practice to multiply the short-time segment with a window which tapers at start and end. Now because the window tappers at the ends, we will make the consecutive short-time segments overlap. This is controlled by ``hopDur``. Below is the code to obtain short-time segments of the signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVBtetEwnqXH"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "def segment_signal(signal, winType='rect', winDur = 25e-3, hopDur=10e-3, sr=16e3):\n",
        "    # hop_size in ms\n",
        "    winLen = int(winDur*sr)\n",
        "    hopLen = int(hopDur*sr)\n",
        "    signal = np.pad(signal, winLen//2, mode='reflect')\n",
        "\n",
        "    nframes = int((len(signal) - winLen) / hopLen) + 1\n",
        "    frames = np.zeros((nframes,winLen))\n",
        "    if winType == 'hamming':\n",
        "        window = np.hamming(winLen)\n",
        "        window = window-np.min(window)\n",
        "        window = window/np.max(window)\n",
        "    elif winType == 'rect':\n",
        "        window = np.ones((winLen,),dtype=float)\n",
        "\n",
        "    for n in range(nframes):\n",
        "        frames[n] = window*signal[n*hopLen:n*hopLen+winLen]\n",
        "    return frames\n",
        "\n",
        "def nearestpow2(n):\n",
        "    k=1\n",
        "    while n>2**k:\n",
        "        k = k+1\n",
        "    return 2**k\n",
        "\n",
        "hopDur = 10e-3 #ms\n",
        "winDur = 25e-3\n",
        "\n",
        "x_segs_rect = segment_signal(x, winType='rect', winDur=winDur, hopDur= hopDur, sr=sr)\n",
        "x_segs_hamming = segment_signal(x, winType='hamming', winDur=winDur, hopDur= hopDur, sr=sr)\n",
        "\n",
        "\n",
        "fix, ax = plt.subplots(2,2,figsize=(14,8))\n",
        "indx = 155\n",
        "nfft = nearestpow2(x_segs_rect.shape[1])\n",
        "axis_freq = np.arange(0,nfft/2+1)*sr/nfft\n",
        "axis_time = indx*hopDur+np.arange(0,x_segs_rect.shape[1],1)/sr\n",
        "\n",
        "X_1 = 10*np.log10(np.abs(np.fft.rfft(x_segs_rect[indx,:],nfft)))\n",
        "X_1 = X_1 - np.max(X_1)\n",
        "X_2 = 10*np.log10(np.abs(np.fft.rfft(x_segs_hamming[indx,:],nfft)))\n",
        "X_2 = X_2 - np.max(X_2)\n",
        "\n",
        "ax[0][0].plot(axis_time,x_segs_rect[indx,:])\n",
        "ax[0][0].grid(True)\n",
        "ax[0][0].set_ylabel('AMPLITUDE [in A.U]', fontsize=14)\n",
        "ax[0][0].set_xlabel('time [in secs]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax[0][0].spines['right'].set_visible(False)\n",
        "ax[0][0].spines['top'].set_visible(False)\n",
        "ax[0][0].text(1.55,0.8,'After rectangular windowing')\n",
        "\n",
        "\n",
        "\n",
        "ax[0][1].plot(axis_time,x_segs_hamming[indx,:])\n",
        "ax[0][1].grid(True)\n",
        "ax[0][1].set_ylabel('AMPLITUDE [in A.U]', fontsize=14)\n",
        "ax[0][1].set_xlabel('time [in secs]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax[0][1].spines['right'].set_visible(False)\n",
        "ax[0][1].spines['top'].set_visible(False)\n",
        "ax[0][1].text(1.55,0.8,'After hamming windowing')\n",
        "\n",
        "ax[1][0].plot(axis_freq,X_1)\n",
        "ax[1][0].grid(True)\n",
        "ax[1][0].set_ylabel('MAGNITUDE SPECTRUM [in dB]', fontsize=14)\n",
        "ax[1][0].set_xlabel('frequency [in Hz]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax[1][0].spines['right'].set_visible(False)\n",
        "ax[1][0].spines['top'].set_visible(False)\n",
        "ax[1][0].text(1.55,0.8,'After rectangular windowing')\n",
        "\n",
        "\n",
        "ax[1][1].plot(axis_freq,X_2)\n",
        "ax[1][1].grid(True)\n",
        "ax[1][1].set_ylabel('MAGNITUDE SPECTRUM [in dB]', fontsize=14)\n",
        "ax[1][1].set_xlabel('frequency [in Hz]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax[1][1].spines['right'].set_visible(False)\n",
        "ax[1][1].spines['top'].set_visible(False)\n",
        "ax[1][1].text(1.55,0.8,'After hamming  windowing')\n",
        "\n",
        "\n",
        "ax[1][0].set_ylim(-40,5)\n",
        "ax[1][1].set_ylim(-40,5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEMddM-UnqXI"
      },
      "source": [
        "The consecutive short-time segments may have significant correlations. Lets visualize this by plotting the covariance matrix of ``x_segs_hamming``. Below we visualize only a part of the correlation matrix. The brighter colors indicate high correlation (diagonal values are 1). We can see some blocks of high correlation. This is likely because the speech signal has burst of vocal activity (spokenn digits), with pauses in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POvaIQSRnqXI"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "x_segs = x_segs_hamming.copy()\n",
        "C = np.zeros((x_segs.shape[0],x_segs.shape[0]))\n",
        "# C = np.dot(x_segs.T,x_segs)\n",
        "C.shape\n",
        "for i in range(x_segs.shape[0]):\n",
        "    for j in range(x_segs.shape[0]):\n",
        "        C[i,j] = np.dot(x_segs[i,:],x_segs[j,:])/np.linalg.norm(x_segs[i,:])/np.linalg.norm(x_segs[j,:])\n",
        "\n",
        "fig = plt.subplots(figsize=(12,6))\n",
        "ax = plt.subplot(1,1,1)\n",
        "# ax.plot(C[110,:])\n",
        "ax.imshow(np.abs(C[100:200,100:200]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaxZnp8EnqXI"
      },
      "source": [
        "## We hear spectral energies\n",
        "\n",
        "### Compute Fourier transform\n",
        "\n",
        "Why? Psychoacoustic and physiology studies of the auditory system of human and animals have shown that we are sensitive to individual frequencies contained in the sound pressure variation. Hence, it makes sense to transform every short-time segment into corresponding Fourier spectrum representation. This is straight forward using the FFT implementation available in python. One thing to focus on here is the ``nfft`` length. This should be at least equal to ``winLen`` to get a meaningful FFT of each segment.\n",
        "\n",
        "### Compute spectral energies\n",
        "The output of FFT is a complex vector. Psychoacoustic studies have shown that our hearing is sensitive to energy distribution across the frequencies. So, we compute the energy by taking the absolute values.\n",
        "Below is the code for the above two things\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHqXQFsBnqXJ"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "X = np.zeros((x_segs.shape[0],int(nfft/2)+1))\n",
        "X = np.abs(np.fft.rfft(x_segs,nfft,axis=1))**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kA3jXK6nqXJ"
      },
      "source": [
        "## Further, we hear in mel-scale\n",
        "\n",
        "### Mel-scale transformation of frequencies\n",
        "There is a phenomenal psychoacoustic work which has attempted to quantify the mapping between the frequency scale obtained from Fourier transform and that perceived by our brain.\n",
        "> [Volkmann and Stevens](https://asa.scitation.org/doi/10.1121/1.1901999) worked on constructing a scale that reflected how people hear musical tones. Listeners were asked to adjust tones so that one tone (a sinusoidal signal of specific frequency) was half as higher as another, and other such subdivisions of the frequency range. This way the mel scale (mel stands for melody) was obtained. You can read more in [this](https://en.wikipedia.org/wiki/Mel_scale) article.\n",
        "\n",
        "A standard definition of mel-scale is: a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference of 1000 mels was assigned as having a frequency of 1000 Hz (at 40 dB above threshold).\n",
        "The below code visualizes this mapping. We can make the following quick observations:\n",
        "* the mapping is non-linear\n",
        "* monotonically increasing in shape\n",
        "* close to linear till 1000 Hz\n",
        "* beyond 1000 Hz it is highly compressive This implies doubling in the linear scale does not result in doubling in the mel-scale!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-qkPnKGnqXJ"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "def freq_to_mel(freq):\n",
        "    # converting linear scale frequency to mel-scale\n",
        "    return 2595.0 * np.log10(1.0 + freq / 700.0)\n",
        "\n",
        "def mel_to_freq(mels):\n",
        "    # converting mel-scale frequency to linear scale\n",
        "    return 700.0 * (10.0**(mels / 2595.0) - 1.0)\n",
        "\n",
        "axis_freqs = np.arange(0,nfft/2+1)/nfft*sr\n",
        "mels = freq_to_mel(axis_freqs)\n",
        "\n",
        "fig = plt.subplots(figsize=[8,5])\n",
        "ax = plt.subplot(1,1,1)\n",
        "ax.plot(axis_freqs,mels, color='tab:red')\n",
        "ax.plot(axis_freqs[:100],axis_freqs[:100],'--',color='gray')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('FREQUENCY [in mels]', fontsize=14)\n",
        "ax.set_xlabel('FREQUENCY [in Hz]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.text(3000,1500,'mel-scale warping',fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdPJYti6nqXJ"
      },
      "source": [
        "### Designing a spectral energy weighting filterbank\n",
        "\n",
        "Remember, our goal is to achieve a compact representation of the speech signal. We have some M (equal to 365) frames, each of dimension 257. Is there a way to reduce the dimension 257 to N, something smaller?\n",
        "\n",
        "Yes, we can make N bins along 1 to 257, and sum the spectral energy in each of these bins. We will get N numbers and thus would have reduced the 257 dimension to N. Now the key questions are:\n",
        "* What should be the size of these bins?\n",
        "* Should these be of uniform sizes from 1 to 257?\n",
        "\n",
        "The frequency warping from linear to mel-scale suggests that non-uniform size bins should be preferred, with the size of the bins gradually increasing. So, we make uniform bins in the mel frequency scale, and this will result in corresponding non-uniform size bins in the linear frequency scale. The figure below shows an illustration for this.\n",
        "![](./my_images/mel_linear_FB.png \" An illustration on short-time features. Here each frame is represented with a 3-D feature vector. Credit:  http://coswara.iisc.ac.in/\")\n",
        "\n",
        "Once we have the binning. Next question is what kind of weighting should we apply to the energies in these bins. A preferred choice is triangular weighting. This will then also imply that we should make the consecutive bins have an overlap 50%. The below code implements these steps and creates the spectral energy weighting filterbank. This is also widely referred to as the mel-scale uniform bandwidth filterbank. Also, it is good to normalize the area under each filter weighting function. This makes the relative peak strength of the filters decrease as we go towards higher center frequency filters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXb6vUGnqXJ"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "freq_min = 0\n",
        "freq_high = sr / 2\n",
        "mel_filter_num = 40\n",
        "\n",
        "def get_filter_points(fmin, fmax, mel_filter_num, nfft, sample_rate=16000):\n",
        "    fmin_mel = freq_to_mel(fmin)\n",
        "    fmax_mel = freq_to_mel(fmax)\n",
        "\n",
        "    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2)\n",
        "    freqs = mel_to_freq(mels)\n",
        "\n",
        "    return np.floor((nfft) / sample_rate * freqs).astype(int), freqs\n",
        "\n",
        "def get_filters(filter_points, nfft):\n",
        "    filters = np.zeros((len(filter_points)-2,int(nfft/2+1)))\n",
        "\n",
        "    for n in range(len(filter_points)-2):\n",
        "        filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])\n",
        "        filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])\n",
        "\n",
        "    return filters\n",
        "\n",
        "\n",
        "filter_points, freqs = get_filter_points(freq_min, freq_high, mel_filter_num, nfft=nfft, sample_rate=sr)\n",
        "filters = get_filters(filter_points, nfft=nfft)\n",
        "\n",
        "fig = plt.subplots(figsize=[16,5])\n",
        "ax = plt.subplot(1,2,1)\n",
        "for n in range(filters.shape[0]):\n",
        "    ax.plot(axis_freqs,filters[n])\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('WEIGHTING [in A.U]', fontsize=14)\n",
        "ax.set_xlabel('FREQUENCY [in Hz]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.text(3000,1.1,'uninform mel-bandwidth filterbank',fontsize=14)\n",
        "\n",
        "\n",
        "# normalizing the filter weighting based on area\n",
        "enorm = 2.0 / (freqs[2:mel_filter_num+2] - freqs[:mel_filter_num])\n",
        "filters *= enorm[:, np.newaxis]\n",
        "\n",
        "ax = plt.subplot(1,2,2)\n",
        "for n in range(filters.shape[0]):\n",
        "    ax.plot(axis_freqs,filters[n])\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('WEIGHTING [in A.U]', fontsize=14)\n",
        "ax.set_xlabel('FREQUENCY [in Hz]', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.text(3000,.0055,'filter area normalized',fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKKaMxRKnqXJ"
      },
      "source": [
        "### Applying the filterbank\n",
        "\n",
        "Once we have the filter bank we apply this to the magnitude FFT ``X`` to transform 257x357 to Nx357. Here, N will be equal to the number of mel filters (``mel_filter_num``) in the filterbank. Also, we apply a ``log`` to the output values. This is because you would have noticed that you can hear faint and loud sounds. Applying log transformation will help us amplify the low energy values and attenuate little bit the high amplitude values (to know more, you can read about it more here - [Weber-Fechner Law](https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oETdfgXnqXK"
      },
      "outputs": [],
      "source": [
        "X_filtered = np.dot(filters, X.T)\n",
        "X_filtered_log = 10.0 * np.log10(X_filtered)\n",
        "\n",
        "fig = plt.subplots(figsize=[16,5])\n",
        "ax = plt.subplot(1,2,1)\n",
        "ax.imshow(10*np.log10(X.T),origin='lower',aspect='auto',cmap='RdBu_r')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('linear frequency FFT bin', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "\n",
        "\n",
        "ax = plt.subplot(1,2,2)\n",
        "ax.imshow(X_filtered_log,origin='lower',aspect='auto',cmap='RdBu_r')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('filter index', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ZPT70RnqXK"
      },
      "source": [
        "## Decorrelation\n",
        "\n",
        "### Decorrelating filterbank outputs\n",
        "\n",
        "The following are some data analysis convenience steps, and (likely) not what happens inside the ear or brain. These steps will help us to further reduce the dimension from Nx357 to Kx357 where K < N. Remember, our goal is to obtain a compact representation.\n",
        "\n",
        "Lets try to see if there is correlation across the filter outputs. If there is, we can reduce the dimension further. How to go ahead? We will do a PCA on ``X_scaled`` and visualize the decrease in explained variance across components. You can see in the figure below it does drop sharply, and likely 5 components suffice to capture most of the data variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZeXHHjinqXK"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "pca = PCA(n_components=X_scaled.shape[0])\n",
        "pca.fit(X_scaled.T)\n",
        "\n",
        "fig = plt.subplots(figsize=[8,5])\n",
        "ax = plt.subplot(1,1,1)\n",
        "ax.plot(pca.explained_variance_ratio_,'-o')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('explained variance', fontsize=14)\n",
        "ax.set_xlabel('PCA component', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R_JA0rXnqXL"
      },
      "source": [
        "Lets also visualize how the principal components look. These look a little oscillatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkL1riONnqXL"
      },
      "outputs": [],
      "source": [
        "fig = plt.subplots(figsize=[12,10])\n",
        "for i in range(5):\n",
        "    ax = plt.subplot(5,1,i+1)\n",
        "    ax.plot(pca.components_[i,:])\n",
        "    ax.set_ylabel('PCA- '+str(i+1), fontsize=14)\n",
        "    plt.xticks(fontsize=13)\n",
        "    plt.yticks(fontsize=13)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qVPapM6nqXL"
      },
      "source": [
        "Drawn by this observation and make our next linear transformation data independent, we can use the discrete cosine transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXvxOsh6nqXM"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "def dct(dct_filter_num, filter_len):\n",
        "    basis = np.empty((dct_filter_num,filter_len))\n",
        "    basis[0, :] = 1.0 / np.sqrt(filter_len)\n",
        "\n",
        "    samples = np.arange(1, 2 * filter_len, 2) * np.pi / (2.0 * filter_len)\n",
        "\n",
        "    for i in range(1, dct_filter_num):\n",
        "        basis[i, :] = np.cos(i * samples) * np.sqrt(2.0 / filter_len)\n",
        "\n",
        "    return basis\n",
        "\n",
        "dct_basis = dct(40,40)\n",
        "fig = plt.subplots(figsize=[16,4])\n",
        "ax = plt.subplot(1,1,1)\n",
        "for i in range(10):\n",
        "    ax.plot(dct_basis[i,:])\n",
        "plt.ylabel('DCT basis amplitudes')\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvlib9B5nqXM"
      },
      "source": [
        "We apply the DCT transform to the ``X_filtered_log`` and the result thus obtained is called MFCCs. The MFCC[0], the first element in the vector obtained after DCT captures the spectral energy across the filterbank, for each short-time frame. This can be seen in the plot below. Interpreting the other dimensions of MFCCs is not straight forward. Also, the visualization does not mean much. But this compact representation is very useful as feature vectors for classification algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9-sV0s4nqXM"
      },
      "outputs": [],
      "source": [
        "#collapse\n",
        "dct_filter_num = X_scaled.shape[0]\n",
        "dct_filters = dct(dct_filter_num, X_filtered_log.shape[0])\n",
        "cepstral_coefficents = np.dot(dct_filters, X_filtered_log)\n",
        "\n",
        "fig = plt.subplots(figsize=[16,5])\n",
        "ax = plt.subplot(1,2,1)\n",
        "ax.imshow(X_filtered_log,origin='lower',aspect='auto',cmap='RdBu_r')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('filter index', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "\n",
        "ax = plt.subplot(1,2,2)\n",
        "ax.imshow(cepstral_coefficents,origin='lower',aspect='auto',cmap='RdBu_r')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('MFCCs', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.subplots(figsize=[16,5])\n",
        "ax = plt.subplot(1,2,1)\n",
        "ax.plot(cepstral_coefficents[0,:])\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('MFCC[0]', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "\n",
        "ax = plt.subplot(1,2,2)\n",
        "ax.imshow(cepstral_coefficents[1:,:],origin='lower',aspect='auto',cmap='RdBu_r')\n",
        "ax.grid(True)\n",
        "ax.set_ylabel('MFCCs[1:39]', fontsize=14)\n",
        "ax.set_xlabel('short-time frame index', fontsize=14)\n",
        "plt.xticks(fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amVRRjuLnqXM"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We obtained a compact representation of the speech signal we started with.\n",
        "* Specifically, the 3.64 seconds signal with 58378 samples is now represented by 40x357 = 14280 samples. This is close to 1/4 of the original sample size. We can reduce it further by just picking only the first 11 MFCCs (as we noticed that the explained variance falls very rapidly, and 11 is also a common choice in the speech processing application domain). We then have a further compact representation of size, 11x357 = 3927, and this is a splendid compression by a factor close to 15x.\n",
        "* Further, our compact representation is based on some insights from psychoacoustic studies.\n",
        "\n",
        "What makes MFCCs popular:\n",
        "* Its implementation is quite easy in a data processing pipeline. It makes use of simple and efficient operations such as FFT, logarithms, and DCT.\n",
        "* The mel-scale is drawn from psychoacoustics studies. It is always inspiring to make use of some operation which is also hypothesized (and proven at least for tones) to be used by our perception.\n",
        "\n",
        "> To give us a feedback please us the comment box below.\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "If you are interested to know more, the below references will be useful.\n",
        "* The excellent discussion section in *L. C. W. Pols, H. R. C. Tromp, and R. Plomp [Frequency analysis of Dutch vowels from 50 male speakers](https://core.ac.uk/download/pdf/205403886.pdf), 1972.*\n",
        "\n",
        "* *Stan W. Davis, P. Mermelstein, [Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences](https://www.semanticscholar.org/paper/Comparison-of-Parametric-Representations-for-Word-Davis-Mermelstein/36cac502dd96788b7eef91bdef152d47b71bd1fb), 1980.*\n",
        "\n",
        "* This [post](https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial) at Kaggle served both as a motivation and improve the understanding to write the notebook. The mel filterbank design code segment is taken from this notebook.\n",
        "\n",
        "* You can find some more posts also [here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html) and [here](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}